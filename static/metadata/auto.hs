[ ( "#active-learning" , ( "" , "" , "" , "" , "" ) )
, ( "#advantages-disadvantages" , ( "" , "" , "" , "" , "" ) )
, ( "#appendix" , ( "" , "" , "" , "" , "" ) )
, ( "#august" , ( "" , "" , "" , "" , "" ) )
, ( "#background" , ( "" , "" , "" , "" , "" ) )
, ( "#bayesian-improvements" , ( "" , "" , "" , "" , "" ) )
, ( "#bradley-terry-preference-learning"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#computational-requirement" , ( "" , "" , "" , "" , "" ) )
, ( "#credits" , ( "" , "" , "" , "" , "" ) )
, ( "#data-the-project-gutenberg-poetry-corpus"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#essay-on-criticism" , ( "" , "" , "" , "" , "" ) )
, ( "#external-links" , ( "" , "" , "" , "" , "" ) )
, ( "#famous-first-lines" , ( "" , "" , "" , "" , "" ) )
, ( "#full-bradley-terry-training" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-345m" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-poetry-prefix-completions"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#gpt-2-poetry-prefix-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-poetry-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-small-generating-poetry" , ( "" , "" , "" , "" , "" ) )
, ( "#hamlet-william-shakespeare" , ( "" , "" , "" , "" , "" ) )
, ( "#howl" , ( "" , "" , "" , "" , "" ) )
, ( "#implementation" , ( "" , "" , "" , "" , "" ) )
, ( "#improvements" , ( "" , "" , "" , "" , "" ) )
, ( "#invictus-william-ernest-henley"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#is-preference-learning-a-bradley-terry-model"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#jabberwocky-lewis-carroll" , ( "" , "" , "" , "" , "" ) )
, ( "#license" , ( "" , "" , "" , "" , "" ) )
, ( "#optimal-exploration" , ( "" , "" , "" , "" , "" ) )
, ( "#overall" , ( "" , "" , "" , "" , "" ) )
, ( "#ozymandias" , ( "" , "" , "" , "" , "" ) )
, ( "#pioneers-o-pioneers-walt-whitman"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#random-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#romeo-juliet-william-shakespeare"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#sailing-to-byzantium-yeats" , ( "" , "" , "" , "" , "" ) )
, ( "#samples" , ( "" , "" , "" , "" , "" ) )
, ( "#section" , ( "" , "" , "" , "" , "" ) )
, ( "#site" , ( "" , "" , "" , "" , "" ) )
, ( "#sonnet-29-shakespeare" , ( "" , "" , "" , "" , "" ) )
, ( "#source-code" , ( "" , "" , "" , "" , "" ) )
, ( "#tao-te-ching" , ( "" , "" , "" , "" , "" ) )
, ( "#the-love-song-of-j.-alfred-prufrock-t.s.-eliot"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#training" , ( "" , "" , "" , "" , "" ) )
, ( "#training-gpt-2-poetry" , ( "" , "" , "" , "" , "" ) )
, ( "#training-gpt-2-poetry-prefix" , ( "" , "" , "" , "" , "" ) )
, ( "#training-gpt-2-small-to-generate-poetry"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#training-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#training-samples-1" , ( "" , "" , "" , "" , "" ) )
, ( "#ulysses-lord-alfred-tennyson" , ( "" , "" , "" , "" , "" ) )
, ( "#unconditional-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#use" , ( "" , "" , "" , "" , "" ) )
, ( "#why-not-bayes" , ( "" , "" , "" , "" , "" ) )
, ( "/About"
  , ( "About This Website"
    , "shawwn"
    , "25 Aug 2019"
    , ""
    , "Meta page describing shawwn.com; copyright license"
    )
  )
, ( "/Faces#fn27"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/Links"
  , ( "Links"
    , "shawwn"
    , "25 Aug 2019"
    , ""
    , "Who am I online & what have I done? - Contact information; sites I use; things I've worked on"
    )
  )
, ( "/Resorter"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/TWDNE"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/Tea#water-experiment"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/Tool-AI"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-03-06-gpt2-poetry-1000samples.txt"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-03-06-gpt2-poetry-prefix-1000samples.txt"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-03-16-gpt2-poetry-prefix-jabberwocky-100samples.txt"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-05-13-gpt2-poetry-345m-5000samples.txt"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-05-24-gpt2-poetry-yeatssecondcoming-500completions.txt"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-07-19-taotehching-ch1-1ksamples.txt"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-07-21-taotehching-all-1ksamples.txt"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-07-22-gpt2-345m-taotehching-all-ch181.tar.xz"
  , ( ""
    , "shawwn"
    , ""
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/culture/2014-kovacs.pdf" , ( "" , "" , "" , "" , "" ) )
, ( "/docs/statistics/comparison/1961-slater.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "/docs/statistics/comparison/2002-pelc.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "/docs/statistics/comparison/2007-karp.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.1421&rep=rep1&type=pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.210.2439&rep=rep1&type=pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://codyraskin.com/research/?p=135"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://creativecommons.org/about/cc0"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://cs.brown.edu/~eli/papers/SICOMP23FRPU.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://datacolada.org/72" , ( "" , "" , "" , "" , "" ) )
, ( "http://dlib.bpums.ac.ir/multiMediaFile/20774386-4-1.pdf#page=111"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://graph.anime.plus/gwern/ratings,anime"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://john-joseph-horton.com/papers/longrun.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Yue_200.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://machinelearning.wustl.edu/mlpapers/paper_files/icml2005_ChuG05.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://machinelearning.wustl.edu/mlpapers/paper_files/icml2014c2_busa-fekete14.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://math.bu.edu/individual/mg/research/glicko.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://papers.nips.cc/paper/5903-online-rank-elicitation-for-plackett-luce-a-dueling-bandits-approach.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://papers.nips.cc/paper/6220-memory-efficient-backpropagation-through-time"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://sevensecularsermons.org/on-the-significance-of-gwerns-poem-generator/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://sfbay-anarchists.org/wp-content/uploads/2012/05/Trurls-Electronic-Bard.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://wiki.obormot.net" , ( "" , "" , "" , "" , "" ) )
, ( "http://www-cgi.cs.cmu.edu/afs/cs.cmu.edu/Web/People/harchol/Papers/SODA94-ranking.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.aclweb.org/anthology/D15-1002"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.astro.cornell.edu/staff/loredo/bayes/bae.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA417190#page=15"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.flickchart.com/" , ( "" , "" , "" , "" , "" ) )
, ( "http://www.jstatsoft.org/v48/i09/paper"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.jstatsoft.org/v48/i10/paper"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.peterbloem.nl/blog/transformers"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.r-inla.org/" , ( "" , "" , "" , "" , "" ) )
, ( "http://youtube-global.blogspot.com/2009/09/five-stars-dominate-ratings.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://arxiv.org/abs/0707.1051"
  , ( "Noisy Sorting Without Resampling"
    , "Mark Braverman, Elchanan Mossel"
    , "2019-08-29"
    , ""
    , "In this paper we study noisy sorting without re-sampling. In this problem there is an unknown order $a_{\\pi(1)} < ... < a_{\\pi(n)}$ where $\\pi$ is a permutation on $n$ elements. The input is the status of $n \\choose 2$ queries of the form $q(a_i,x_j)$, where $q(a_i,a_j) = +$ with probability at least $1/2+\\ga$ if $\\pi(i) > \\pi(j)$ for all pairs $i \\neq j$, where $\\ga > 0$ is a constant and $q(a_i,a_j) = -q(a_j,a_i)$ for all $i$ and $j$. It is assumed that the errors are independent. Given the status of the queries the goal is to find the maximum likelihood order. In other words, the goal is find a permutation $\\sigma$ that minimizes the number of pairs $\\sigma(i) > \\sigma(j)$ where $q(\\sigma(i),\\sigma(j)) = -$. The problem so defined is the feedback arc set problem on distributions of inputs, each of which is a tournament obtained as a noisy perturbations of a linear order. Note that when $\\ga < 1/2$ and $n$ is large, it is impossible to recover the original order $\\pi$.   It is known that the weighted feedback are set problem on tournaments is NP-hard in general. Here we present an algorithm of running time $n^{O(\\gamma^{-4})}$ and sampling complexity $O_{\\gamma}(n \\log n)$ that with high probability solves the noisy sorting without re-sampling problem. We also show that if $a_{\\sigma(1)},a_{\\sigma(2)},...,a_{\\sigma(n)}$ is an optimal solution of the problem then it is ``close'' to the original order. More formally, with high probability it holds that $\\sum_i |\\sigma(i) - \\pi(i)| = \\Theta(n)$ and $\\max_i |\\sigma(i) - \\pi(i)| = \\Theta(\\log n)$.   Our results are of interest in applications to ranking, such as ranking in sports, or ranking of search items based on comparisons by experts."
    )
  )
, ( "https://arxiv.org/abs/0910.1191"
  , ( "Sorting from Noisy Information"
    , "Mark Braverman, Elchanan Mossel"
    , "2019-08-29"
    , ""
    , "This paper studies problems of inferring order given noisy information. In these problems there is an unknown order (permutation) $\\pi$ on $n$ elements denoted by $1,...,n$. We assume that information is generated in a way correlated with $\\pi$. The goal is to find a maximum likelihood $\\pi^*$ given the information observed. We will consider two different types of observations: noisy comparisons and noisy orders. The data in Noisy orders are permutations given from an exponential distribution correlated with \\pi (this is also called the Mallow's model). The data in Noisy Comparisons is a signal given for each pair of elements which is correlated with their true ordering.   In this paper we present polynomial time algorithms for solving both problems with high probability. As part of our proof we show that for both models the maximum likelihood solution $\\pi^{\\ast}$ is close to the original permutation $\\pi$.   Our results are of interest in applications to ranking, such as ranking in sports, or ranking of search items based on comparisons by experts."
    )
  )
, ( "https://arxiv.org/abs/1112.5745"
  , ( "Bayesian Active Learning for Classification and Preference Learning"
    , "Neil Houlsby, Ferenc Husz\225r, Zoubin Ghahramani, M\225t\233 Lengyel"
    , "2019-08-29"
    , ""
    , "Information theoretic active learning has been widely studied for probabilistic models. For simple regression an optimal myopic policy is easily tractable. However, for other tasks and with more complex models, such as classification with nonparametric models, the optimal solution is harder to compute. Current approaches make approximations to achieve tractability. We propose an approach that expresses information gain in terms of predictive entropies, and apply this method to the Gaussian Process Classifier (GPC). Our approach makes minimal approximations to the full information theoretic objective. Our experimental performance compares favourably to many popular active learning algorithms, and has equal or lower computational complexity. We compare well to decision theoretic approaches also, which are privy to more information and require much more computational time. Secondly, by developing further a reformulation of binary preference learning to a classification problem, we extend our algorithm to Gaussian Process preference learning."
    )
  )
, ( "https://arxiv.org/abs/1502.05556"
  , ( "Just Sort It! A Simple and Effective Approach to Active Preference\n  Learning"
    , "Lucas Maystre, Matthias Grossglauser"
    , "2019-08-29"
    , ""
    , "We address the problem of learning a ranking by using adaptively chosen pairwise comparisons. Our goal is to recover the ranking accurately but to sample the comparisons sparingly. If all comparison outcomes are consistent with the ranking, the optimal solution is to use an efficient sorting algorithm, such as Quicksort. But how do sorting algorithms behave if some comparison outcomes are inconsistent with the ranking? We give favorable guarantees for Quicksort for the popular Bradley-Terry model, under natural assumptions on the parameters. Furthermore, we empirically demonstrate that sorting algorithms lead to a very simple and effective active learning strategy: repeatedly sort the items. This strategy performs as well as state-of-the-art methods (and much better than random sampling) at a minuscule fraction of the computational cost."
    )
  )
, ( "https://arxiv.org/abs/1604.06174"
  , ( "Training Deep Nets with Sublinear Memory Cost"
    , "Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin"
    , "2019-08-29"
    , ""
    , "We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences."
    )
  )
, ( "https://arxiv.org/abs/1610.01945"
  , ( "Connecting Generative Adversarial Networks and Actor-Critic Methods"
    , "David Pfau, Oriol Vinyals"
    , "2019-08-29"
    , ""
    , "Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities."
    )
  )
, ( "https://arxiv.org/abs/1611.03852"
  , ( "A Connection between Generative Adversarial Networks, Inverse\n  Reinforcement Learning, and Energy-Based Models"
    , "Chelsea Finn, Paul Christiano, Pieter Abbeel, Sergey Levine"
    , "2019-08-29"
    , ""
    , "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains."
    )
  )
, ( "https://arxiv.org/abs/1706.03741"
  , ( "Deep reinforcement learning from human preferences"
    , "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei"
    , "2019-08-29"
    , ""
    , "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback."
    )
  )
, ( "https://arxiv.org/abs/1706.03762"
  , ( "Attention Is All You Need"
    , "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
    , "2019-08-29"
    , ""
    , "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
    )
  )
, ( "https://arxiv.org/abs/1706.03799"
  , ( "Verb Physics: Relative Physical Knowledge of Actions and Objects"
    , "Maxwell Forbes, Yejin Choi"
    , "2019-08-29"
    , ""
    , "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., \"My house is bigger than me.\" However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, \"Tyler entered his house\" implies that his house is bigger than Tyler.   In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance."
    )
  )
, ( "https://arxiv.org/abs/1706.07068"
  , ( "CAN: Creative Adversarial Networks, Generating \"Art\" by Learning About\n  Styles and Deviating from Style Norms"
    , "Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, Marian Mazzone"
    , "2019-08-29"
    , ""
    , "We propose a new system for generating art. The system generates art by looking at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such networks are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs. Human subjects even rated the generated images higher on various scales."
    )
  )
, ( "https://arxiv.org/abs/1707.09971"
  , ( "Spectral Method and Regularized MLE Are Both Optimal for Top-$K$ Ranking"
    , "Yuxin Chen, Jianqing Fan, Cong Ma, Kaizheng Wang"
    , "2019-08-29"
    , "10.1214/18-AOS1745"
    , "This paper is concerned with the problem of top-$K$ ranking from pairwise comparisons. Given a collection of $n$ items and a few pairwise comparisons across them, one wishes to identify the set of $K$ items that receive the highest ranks. To tackle this problem, we adopt the logistic parametric model --- the Bradley-Terry-Luce model, where each item is assigned a latent preference score, and where the outcome of each pairwise comparison depends solely on the relative scores of the two items involved. Recent works have made significant progress towards characterizing the performance (e.g. the mean square error for estimating the scores) of several classical methods, including the spectral method and the maximum likelihood estimator (MLE). However, where they stand regarding top-$K$ ranking remains unsettled.   We demonstrate that under a natural random sampling model, the spectral method alone, or the regularized MLE alone, is minimax optimal in terms of the sample complexity --- the number of paired comparisons needed to ensure exact top-$K$ identification, for the fixed dynamic range regime. This is accomplished via optimal control of the entrywise error of the score estimates. We complement our theoretical studies by numerical experiments, confirming that both methods yield low entrywise errors for estimating the underlying scores. Our theory is established via a novel leave-one-out trick, which proves effective for analyzing both iterative and non-iterative procedures. Along the way, we derive an elementary eigenvector perturbation bound for probability transition matrices, which parallels the Davis-Kahan $\\sin\\Theta$ theorem for symmetric matrices. This also allows us to close the gap between the $\\ell_2$ error upper bound for the spectral method and the minimax lower limit."
    )
  )
, ( "https://arxiv.org/abs/1709.06390"
  , ( "Analogical-based Bayesian Optimization"
    , "Trung Le, Khanh Nguyen, Tu Dinh Nguyen, Dinh Phung"
    , "2019-08-29"
    , ""
    , "Some real-world problems revolve to solve the optimization problem \\max_{x\\in\\mathcal{X}}f\\left(x\\right) where f\\left(.\\right) is a black-box function and X might be the set of non-vectorial objects (e.g., distributions) where we can only define a symmetric and non-negative similarity score on it. This setting requires a novel view for the standard framework of Bayesian Optimization that generalizes the core insightful spirit of this framework. With this spirit, in this paper, we propose Analogical-based Bayesian Optimization that can maximize black-box function over a domain where only a similarity score can be defined. Our pathway is as follows: we first base on the geometric view of Gaussian Processes (GP) to define the concept of influence level that allows us to analytically represent predictive means and variances of GP posteriors and base on that view to enable replacing kernel similarity by a more genetic similarity score. Furthermore, we also propose two strategies to find a batch of query points that can efficiently handle high dimensional data."
    )
  )
, ( "https://arxiv.org/abs/1709.06683"
  , ( "OptionGAN: Learning Joint Reward-Policy Options using Generative\n  Adversarial Inverse Reinforcement Learning"
    , "Peter Henderson, Wei-Di Chang, Pierre-Luc Bacon, David Meger, Joelle Pineau, Doina Precup"
    , "2019-08-29"
    , ""
    , "Reinforcement learning has shown promise in learning policies that can solve complex problems. However, manually specifying a good reward function can be difficult, especially for intricate tasks. Inverse reinforcement learning offers a useful paradigm to learn the underlying reward function directly from expert demonstrations. Yet in reality, the corpus of demonstrations may contain trajectories arising from a diverse set of underlying reward functions rather than a single one. Thus, in inverse reinforcement learning, it is useful to consider such a decomposition. The options framework in reinforcement learning is specifically designed to decompose policies in a similar light. We therefore extend the options framework and propose a method to simultaneously recover reward options in addition to policy options. We leverage adversarial methods to learn joint reward-policy options using only observed expert states. We show that this approach works well in both simple and complex continuous control tasks and shows significant performance increases in one-shot transfer learning."
    )
  )
, ( "https://arxiv.org/abs/1802.01241"
  , ( "Semantic projection: recovering human knowledge of multiple, distinct\n  object features from word embeddings"
    , "Gabriel Grand, Idan Asher Blank, Francisco Pereira, Evelina Fedorenko"
    , "2019-08-29"
    , ""
    , "The words of a language reflect the structure of the human mind, allowing us to transmit thoughts between individuals. However, language can represent only a subset of our rich and detailed cognitive architecture. Here, we ask what kinds of common knowledge (semantic memory) are captured by word meanings (lexical semantics). We examine a prominent computational model that represents words as vectors in a multidimensional space, such that proximity between word-vectors approximates semantic relatedness. Because related words appear in similar contexts, such spaces - called \"word embeddings\" - can be learned from patterns of lexical co-occurrences in natural language. Despite their popularity, a fundamental concern about word embeddings is that they appear to be semantically \"rigid\": inter-word proximity captures only overall similarity, yet human judgments about object similarities are highly context-dependent and involve multiple, distinct semantic features. For example, dolphins and alligators appear similar in size, but differ in intelligence and aggressiveness. Could such context-dependent relationships be recovered from word embeddings? To address this issue, we introduce a powerful, domain-general solution: \"semantic projection\" of word-vectors onto lines that represent various object features, like size (the line extending from the word \"small\" to \"big\"), intelligence (from \"dumb\" to \"smart\"), or danger (from \"safe\" to \"dangerous\"). This method, which is intuitively analogous to placing objects \"on a mental scale\" between two extremes, recovers human judgments across a range of object categories and properties. We thus show that word embeddings inherit a wealth of common knowledge from word co-occurrence statistics and can be flexibly manipulated to express context-dependent meanings."
    )
  )
, ( "https://arxiv.org/abs/1802.06942"
  , ( "Comparison Based Learning from Weak Oracles"
    , "Ehsan Kazemi, Lin Chen, Sanjoy Dasgupta, Amin Karbasi"
    , "2019-08-29"
    , ""
    , "There is increasing interest in learning algorithms that involve interaction between human and machine. Comparison-based queries are among the most natural ways to get feedback from humans. A challenge in designing comparison-based interactive learning algorithms is coping with noisy answers. The most common fix is to submit a query several times, but this is not applicable in many situations due to its prohibitive cost and due to the unrealistic assumption of independent noise in different repetitions of the same query.   In this paper, we introduce a new weak oracle model, where a non-malicious user responds to a pairwise comparison query only when she is quite sure about the answer. This model is able to mimic the behavior of a human in noise-prone regions. We also consider the application of this weak oracle model to the problem of content search (a variant of the nearest neighbor search problem) through comparisons. More specifically, we aim at devising efficient algorithms to locate a target object in a database equipped with a dissimilarity metric via invocation of the weak comparison oracle. We propose two algorithms termed WORCS-I and WORCS-II (Weak-Oracle Comparison-based Search), which provably locate the target object in a number of comparisons close to the entropy of the target distribution. While WORCS-I provides better theoretical guarantees, WORCS-II is applicable to more technically challenging scenarios where the algorithm has limited access to the ranking dissimilarity between objects. A series of experiments validate the performance of our proposed algorithms."
    )
  )
, ( "https://arxiv.org/abs/1803.02155"
  , ( "Self-Attention with Relative Position Representations"
    , "Peter Shaw, Jakob Uszkoreit, Ashish Vaswani"
    , "2019-08-29"
    , ""
    , "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs."
    )
  )
, ( "https://arxiv.org/abs/1808.04444"
  , ( "Character-Level Language Modeling with Deeper Self-Attention"
    , "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones"
    , "2019-08-29"
    , ""
    , "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions."
    )
  )
, ( "https://arxiv.org/abs/1901.02860"
  , ( "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
    , "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
    , "2019-08-29"
    , ""
    , "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
    )
  )
, ( "https://arxiv.org/abs/1904.09751"
  , ( "The Curious Case of Neural Text Degeneration"
    , "Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi"
    , "2019-08-29"
    , ""
    , "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.   In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model.   Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence."
    )
  )
, ( "https://arxiv.org/abs/1904.10509"
  , ( "Generating Long Sequences with Sparse Transformers"
    , "Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever"
    , "2019-08-29"
    , ""
    , "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more."
    )
  )
, ( "https://arxiv.org/abs/1905.02175"
  , ( "Adversarial Examples Are Not Bugs, They Are Features"
    , "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry"
    , "2019-08-29"
    , ""
    , "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data."
    )
  )
, ( "https://arxiv.org/abs/1905.03197"
  , ( "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation"
    , "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon"
    , "2019-08-29"
    , ""
    , "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling objectives: unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. We can fine-tune UniLM as a unidirectional decoder, a bidirectional encoder, or a sequence-to-sequence model to support various downstream natural language understanding and generation tasks.   UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, our model achieves new state-of-the-art results on three natural language generation tasks, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.63 (2.16 absolute improvement), pushing the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), and the SQuAD question generation BLEU-4 to 22.88 (6.50 absolute improvement)."
    )
  )
, ( "https://arxiv.org/abs/1905.12616"
  , ( "Defending Against Neural Fake News"
    , "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi"
    , "2019-08-29"
    , ""
    , "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.   Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.   Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news."
    )
  )
, ( "https://arxiv.org/abs/1906.08237"
  , ( "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
    , "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
    , "2019-08-29"
    , ""
    , "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking."
    )
  )
, ( "https://arxiv.org/abs/1908.04319"
  , ( "Neural Text Generation with Unlikelihood Training"
    , "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston"
    , "2019-08-29"
    , ""
    , "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive responses. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model itself are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences that contain repeats and frequent words unlike the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving far superior generations using standard greedy or beam search. Our approach provides a strong alternative to traditional training."
    )
  )
, ( "https://arxiv.org/pdf/1706.03741.pdf#page=15"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://arxiv.org/pdf/1809.11096.pdf#page=6"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://ask-gpt.tumblr.com/" , ( "" , "" , "" , "" , "" ) )
, ( "https://ask-gpt.tumblr.com/post/183402346117/april-is-the-cruelest-month-breeding-n-lilacs"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://bitbucket.org/djhshih/argparser"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://blog.floydhub.com/the-transformer-in-pytorch/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://boingboing.net/2019/03/15/digital-lit.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://cran.r-project.org/web/packages/BradleyTerry2/index.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://cs.brown.edu/research/pubs/theses/masters/2007/schudy.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://decaut.org/situ/index.php/ttc-compilation/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://en.wikipedia.org/wiki/Amazon_S3"
  , ( "Amazon S3"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Amazon S3</b> or <b>Amazon Simple Storage Service</b> is a service offered by Amazon Web Services (AWS) that provides object storage through a web service interface. Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its global e-commerce network.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Bouba-Kiki_effect"
  , ( "Bouba/kiki effect"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <b>bouba/kiki effect</b> is a non-arbitrary mapping between speech sounds and the visual shape of objects. This effect was first observed by German-American psychologist Wolfgang K\246hler in 1929. In psychological experiments first conducted on the island of Tenerife, K\246hler showed forms similar to those shown at the right and asked participants which shape was called \"takete\" and which was called \"baluba\". Although not explicitly stated, K\246hler implies that there was a strong preference to pair the jagged shape with \"takete\" and the rounded shape with \"baluba\".</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Bradley-Terry_model"
  , ( "Bradley\8211Terry model"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <b>Bradley\8211Terry model</b> is a probability model that can predict the outcome of a paired comparison. Given a pair of individuals <span class=\"texhtml mvar\" style=\"font-style:italic\">i</span> and <span class=\"texhtml mvar\" style=\"font-style:italic\">j</span> drawn from some population, it estimates the probability that the pairwise comparison <span class=\"texhtml \"><i>i</i> &gt; <i>j</i></span> turns out true, as</p><dl><dd><span class=\"mwe-math-element\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b702a5b9217ab1a685e9c16743ee8ef98ccf20b5\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align:-2.505ex;width:19.237ex;height:5.509ex\" /></span>\n</dd></dl>"
    )
  )
, ( "https://en.wikipedia.org/wiki/CloudFlare"
  , ( "Cloudflare"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Cloudflare, Inc.</b> is an American web infrastructure and website security company, providing content delivery network services, DDoS mitigation, Internet security, and distributed domain name server services. Cloudflare's services sit between a website's visitor and the Cloudflare user's hosting provider, acting as a reverse proxy for websites. Cloudflare's headquarters are in San Francisco, California, with additional offices in Lisbon, London, Singapore, Munich, San Jose, Champaign, Illinois, Austin, New York City and Washington, D.C.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Content_delivery_network"
  , ( "Content delivery network"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>A <b>content delivery network</b> or <b>content distribution network</b> (<b>CDN</b>) is a geographically distributed network of proxy servers and their data centers. The goal is to provide high availability and high performance by distributing the service spatially relative to end-users. CDNs serve a large portion of the Internet content today, including web objects, downloadable objects, applications, live streaming media, on-demand streaming media, and social media sites.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Creative_Commons"
  , ( "Creative Commons"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Creative Commons</b> (<b>CC</b>) is an American non-profit organization devoted to expanding the range of creative works available for others to build upon legally and to share. The organization has released several copyright-licenses, known as Creative Commons licenses, free of charge to the public. These licenses allow creators to communicate which rights they reserve and which rights they waive for the benefit of recipients or other creators. An easy-to-understand one-page explanation of rights, with associated visual symbols, explains the specifics of each Creative Commons license. Creative Commons licenses do not replace copyright but are based upon it. They replace individual negotiations for specific rights between copyright owner (licensor) and licensee, which are necessary under an \"all rights reserved\" copyright management, with a \"some rights reserved\" management employing standardized licenses for re-use cases where no commercial compensation is sought by the copyright owner. The result is an agile, low-overhead and low-cost copyright-management regime, benefiting both copyright owners and licensees.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Decision_fatigue"
  , ( "Decision fatigue"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>In decision making and psychology, <b>decision fatigue</b> refers to the deteriorating quality of decisions made by an individual after a long session of decision making. It is now understood as one of the causes of irrational trade-offs in decision making. For instance, judges in court have been shown to make poorer quality decisions late in the day than they do early in the day. Decision fatigue may also lead to consumers making poor choices with their purchases.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Elegy_Written_in_a_Country_Churchyard"
  , ( "Elegy Written in a Country Churchyard"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>Elegy Written in a Country Churchyard</b></i> is a poem by Thomas Gray, completed in 1750 and first published in 1751. The poem's origins are unknown, but it was partly inspired by Gray's thoughts following the death of the poet Richard West in 1742. Originally titled <i>Stanzas Wrote in a Country Church-Yard</i>, the poem was completed when Gray was living near St Giles' parish church at Stoke Poges. It was sent to his friend Horace Walpole, who popularised the poem among London literary circles. Gray was eventually forced to publish the work on 15 February 1751 in order to preempt a magazine publisher from printing an unlicensed copy of the poem.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Elo_rating_system"
  , ( "Elo rating system"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <b>Elo rating system</b> is a method for calculating the relative skill levels of players in zero-sum games such as chess. It is named after its creator Arpad Elo, a Hungarian-American physics professor.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Essay_On_Criticism"
  , ( "An Essay on Criticism"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>An Essay on Criticism</b></i> is one of the first major poems written by the English writer Alexander Pope (1688\8211\&1744). It is the source of the famous quotations \"To err is human, to forgive divine,\" \"A little learning is a dang'rous thing\", and \"Fools rush in where angels fear to tread.\" It first appeared in 1711 after having been written in 1709, and it is clear from Pope's correspondence that many of the poem's ideas had existed in prose form since at least 1706. Composed in heroic couplets and written in the Horatian mode of satire, it is a verse essay primarily concerned with how writers and critics behave in the new literary commerce of Pope's contemporary age. The poem covers a range of good criticism and advice, and represents many of the chief literary ideals of Pope's age.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/GoodReads"
  , ( "Goodreads"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Goodreads</b> is a social cataloging website that allows individuals to freely search its database of books, annotations, and reviews. Users can sign up and register books to generate library catalogs and reading lists. They can also create their own groups of book suggestions, surveys, polls, blogs, and discussions. The website's offices are located in San Francisco. The company is owned by the online retailer Amazon.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Invictus"
  , ( "Invictus"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>\"<b>Invictus</b>\" is a short Victorian poem by the English poet William Ernest Henley (1849\8211\&1903). It was written in 1875 and published in 1888 in his first volume of poems, <i>Book of Verses</i>, in the section <i>Life and Death (Echoes)</i>. It shows how Henley never lost hope and kept faith in himself and faced the struggles unafraid.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Jabberwocky"
  , ( "Jabberwocky"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>\"<b>Jabberwocky</b>\" is a nonsense poem written by Lewis Carroll about the killing of a creature named \"the Jabberwock\". It was included in his 1871 novel <i>Through the Looking-Glass, and What Alice Found There</i>, the sequel to <i>Alice's Adventures in Wonderland</i>. The book tells of Alice's adventures within the back-to-front world of Looking-Glass Land.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Kalevala"
  , ( "Kalevala"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Kalevala</b></i> is a 19th-century work of epic poetry compiled by Elias L\246nnrot from Karelian and Finnish oral folklore and mythology.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Ozymandias"
  , ( "Ozymandias"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>\"Ozymandias\"</b> is the title of two related sonnets published in 1818.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Pioneers%21_O_Pioneers%21"
  , ( "Pioneers! O Pioneers!"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>\"<b>Pioneers! O Pioneers!</b>\" is a poem by the American poet Walt Whitman. It was first published in <i>Leaves of Grass</i> in 1865. The poem was written as a tribute to Whitman's fervor for the great Westward expansion in the United States that led to things like the California Gold Rush and exploration of the far west.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Sonnet_29"
  , ( "Sonnet 29"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Sonnet 29</b> is one of 154 sonnets written by the English playwright and poet William Shakespeare. It is part of the Fair Youth sequence. In the sonnet, the speaker bemoans his status as an outcast and failure but feels better upon thinking of his beloved. Sonnet 29 is written in the typical Shakespearean sonnet form, having 14 lines of iambic pentameter ending in a rhymed couplet.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Tao_Te_Ching"
  , ( "Tao Te Ching"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <i><b>Tao Te Ching</b></i>, Chinese: \36947\24503\32463; pinyin: <i><span><b>Dao De Jing</b></span></i>), also known as <i><b>Lao Tzu</b></i> or <i><b>Laozi</b></i>, is a Chinese classic text traditionally credited to the 6th-century BC sage Laozi. The text's authorship, date of composition and date of compilation are debated. The oldest excavated portion dates back to the late 4th century BC, but modern scholarship dates other parts of the text as having been written\8212or at least compiled\8212later than the earliest portions of the <i>Zhuangzi</i>.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/The_Cyberiad"
  , ( "The Cyberiad"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Cyberiad</b></i> is a series of humorous science fiction short stories by Polish writer Stanis\322aw Lem, originally published in 1965, with an English translation appearing in 1974. The main protagonists of the series are Trurl and Klapaucius, the \"constructors\".</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/The_Rime_of_the_Ancient_Mariner"
  , ( "The Rime of the Ancient Mariner"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Rime of the Ancient Mariner</b></i> is the longest major poem by the English poet Samuel Taylor Coleridge, written in 1797\8211\&98 and published in 1798 in the first edition of <i>Lyrical Ballads</i>. Some modern editions use a revised version printed in 1817 that featured a gloss. Along with other poems in <i>Lyrical Ballads</i>, it is often considered a signal shift to modern poetry and the beginning of British Romantic literature.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/The_Song_of_Hiawatha"
  , ( "The Song of Hiawatha"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Song of Hiawatha</b></i> is an 1855 epic poem in trochaic tetrameter by Henry Wadsworth Longfellow which features Native American characters. The epic relates the fictional adventures of an Ojibwe warrior named Hiawatha and the tragedy of his love for Minnehaha, a Dakota woman. Events in the story are set in the Pictured Rocks area on the south shore of Lake Superior. Longfellow's poem is based on oral traditions surrounding the figure of Manabozho, but it also contains his own innovations.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Thomas_Gray"
  , ( "Thomas Gray"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Thomas Gray</b> was an English poet, letter-writer, classical scholar, and professor at Pembroke College, Cambridge. He is widely known for his <i>Elegy Written in a Country Churchyard,</i> published in 1751.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/TrueSkill"
  , ( "TrueSkill"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>TrueSkill</b> is a skill-based ranking system developed by Microsoft for use with video game matchmaking on Xbox Live. Unlike the popular Elo rating system, which was initially designed for chess, TrueSkill is designed to support games with more than two players.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/adaptive_sort"
  , ( "Adaptive sort"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>A sorting algorithm falls into the <b>adaptive sort</b> family if it takes advantage of existing order in its input. It benefits from the presortedness in the input sequence \8211 or a limited amount of disorder for various definitions of measures of disorder \8211 and sorts faster. Adaptive sorting is usually performed by modifying existing sorting algorithms.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/beam_search"
  , ( "Beam search"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>In computer science, <b>beam search</b> is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates. It is thus a greedy algorithm.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/grade_inflation"
  , ( "Grade inflation"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Grade inflation</b> is used in two senses: (1) grading leniency: the awarding of higher grades than students deserve, which yields a higher average grade given to students; and (2) the tendency to award progressively higher academic grades for work that would have received lower grades in the past.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/latent_variable"
  , ( "Latent variable"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>In statistics, <b>latent variables</b>, are variables that are not directly observed but are rather inferred from other variables that are observed. Mathematical models that aim to explain observed variables in terms of latent variables are called latent variable models. Latent variable models are used in many disciplines, including psychology, demography, economics, engineering, medicine, physics, machine learning/artificial intelligence, bioinformatics, chemometrics, natural language processing, econometrics, management and the social sciences.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/liability-threshold_model"
  , ( "Threshold model"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>In mathematical or statistical modeling a <b>threshold model</b> is any model where a threshold value, or set of threshold values, is used to distinguish ranges of values where the behaviour predicted by the model varies in some important way. A particularly important instance arises in toxicology, where the model for the effect of a drug may be that there is zero effect for a dose below a critical or threshold value, while an effect of some significance exists above that value. Certain types of regression model may include threshold effects.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/standard_error"
  , ( "Standard error"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <b>standard error</b> (<b>SE</b>) of a statistic is the standard deviation of its sampling distribution or an estimate of that standard deviation. If the parameter or the statistic is the mean, it is called the <b>standard error of the mean</b> (<b>SEM</b>).</p>"
    )
  )
, ( "https://github.com/ConnorJL/GPT2"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/aparrish/gutenberg-poetry-corpus"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/dariusk/wordfilter"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/google/python-fire/issues/168"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/karpathy/char-rnn/issues/138"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/minimaxir/gpt-2-keyword-generation"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/nshepperd/gpt-2"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/nxskok/btstan/blob/master/R/btstan.R"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/openai/gpt-2/blob/master/gpt2-samples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/openai/gpt-2/issues/19"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/shawwn/wiki" , ( "" , "" , "" , "" , "" ) )
, ( "https://hal.archives-ouvertes.fr/hal-01972948/document"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://jalammar.github.io/illustrated-transformer/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/@mwichary/seduced-shaggy-samson-snored-725b5a8086d9"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!HXhRwS7R!yl4qZM-gMWdn4Qc3scavOBKqdLNAcZ_WYd2gVPqabPg"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!XMl3UI7b!KNLvp5wuxe_WAgJwkMVDiyyNmDl9XDXuipl-dQ6Phow"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!m5FWGCgZ!cjvMgViPbBqep_6HqYDb2D3Kl8Tt-RsUnwg7457IfDk"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!zX4lzCzK!TNo_1uDlvszGkBUEdd5R_cQ-7Dfv0gyaaaq8BVzw1jA"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://myanimelist.net/" , ( "" , "" , "" , "" , "" ) )
, ( "https://news.ycombinator.com/item?id=19399467"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/better-language-models/#update"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/musenet/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/sparse-transformer/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://pali6.github.io/computer-generated-foundation/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://pdfs.semanticscholar.org/2a25/80233a5e23ca06dcd96fa1e037d014848360.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://reddit.com/r/SubSimulatorGPT2Meta/comments/ccvspt/update_experimenting_with_generating_hybrid/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://slatestarcodex.com/2019/03/14/gwerns-ai-generated-poetry/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://stackroboflow.com/" , ( "" , "" , "" , "" , "" ) )
, ( "https://svilentodorov.xyz/blog/gpt-finetune"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://toolbox.google.com/datasetsearch/search?query=poem%20OR%20poetry&docid=T3haTlmLU9Dl6xqYAAAAAA%3D%3D"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://twitter.com/HW" , ( "" , "" , "" , "" , "" ) )
, ( "https://twitter.com/theshawwn/lists/machine-learning"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://twitter.com/theshawwn/lists/machine-learning/members"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://variety.com/2017/digital/news/netflix-thumbs-vs-stars-1202010492/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://web.archive.org/web/20160102165131/http://bayes.bgsu.edu/webinar.11.2012/R%20output/Rcode.part4.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gutenberg.org/ebooks/24560"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gutenberg.org/ebooks/author/907"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net" , ( "" , "" , "" , "" , "" ) )
, ( "https://www.gwern.net/About#design"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-03-06-gpt2-poetry-1000samples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-03-06-gpt2-poetry-prefix-1000samples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-03-16-gpt2-poetry-prefix-jabberwocky-100samples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-05-13-gpt2-poetry-345m-5000samples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-05-24-gpt2-poetry-yeatssecondcoming-500completions.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-07-19-taotehching-ch1-1ksamples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-07-21-taotehching-all-1ksamples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/ai/2019-07-22-gpt2-345m-taotehching-all-ch181.tar.xz"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/culture/2014-kovacs.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/statistics/comparison/1961-slater.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/statistics/comparison/2002-pelc.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net/docs/statistics/comparison/2007-karp.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.kaggle.com/ultrajack/modern-renaissance-poetry"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.poetryfoundation.org/harriet/2013/03/the-average-fourth-grader-is-a-better-poet-than-you-and-me-too"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.poetryfoundation.org/poems/45146/to-a-skylark"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/MachineLearning/comments/coc09l/p_these_lyrics_do_not_exist/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/SubSimulatorGPT2/comments/btfhks/what_is_rsubsimulatorgpt2/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/gwern/comments/asb1va/an_eternal_howl_gpt2_completions_of_allen/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/slatestarcodex/comments/as8ke7/an_eternal_howl/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/slatestarcodex/comments/b1b47h/gwerns_aigenerated_poetry/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/user/starspawn0"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.thisstorydoesnotexist.com/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.vox.com/2019/5/15/18623134/openai-language-ai-gpt2-poetry-try-it"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.wpi.edu/Pubs/ETD/Available/etd-050505-154305/unrestricted/shawnhal.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.youtube.com/watch?v=iu6eWy7BKKI"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://xkcd.com/1098/" , ( "" , "" , "" , "" , "" ) )
, ( "https://xkcd.com/325/" , ( "" , "" , "" , "" , "" ) )
, ( "https://youtu.be/bIrEM2FbOLU?t=2740"
  , ( "" , "" , "" , "" , "" )
  )
]