[ ( "#active-learning" , ( "" , "" , "" , "" , "" ) )
, ( "#advantages-disadvantages" , ( "" , "" , "" , "" , "" ) )
, ( "#appendix" , ( "" , "" , "" , "" , "" ) )
, ( "#august" , ( "" , "" , "" , "" , "" ) )
, ( "#bradley-terry-preference-learning"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#credits" , ( "" , "" , "" , "" , "" ) )
, ( "#data-the-project-gutenberg-poetry-corpus"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#essay-on-criticism" , ( "" , "" , "" , "" , "" ) )
, ( "#external-links" , ( "" , "" , "" , "" , "" ) )
, ( "#famous-first-lines" , ( "" , "" , "" , "" , "" ) )
, ( "#full-bradley-terry-training" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-345m" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-poetry-prefix-completions"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#gpt-2-poetry-prefix-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-poetry-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#gpt-2-small-generating-poetry" , ( "" , "" , "" , "" , "" ) )
, ( "#hamlet-william-shakespeare" , ( "" , "" , "" , "" , "" ) )
, ( "#howl" , ( "" , "" , "" , "" , "" ) )
, ( "#improvements" , ( "" , "" , "" , "" , "" ) )
, ( "#invictus-william-ernest-henley"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#is-preference-learning-a-bradley-terry-model"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#jabberwocky-lewis-carroll" , ( "" , "" , "" , "" , "" ) )
, ( "#license" , ( "" , "" , "" , "" , "" ) )
, ( "#overall" , ( "" , "" , "" , "" , "" ) )
, ( "#ozymandias" , ( "" , "" , "" , "" , "" ) )
, ( "#pioneers-o-pioneers-walt-whitman"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#random-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#romeo-juliet-william-shakespeare"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#sailing-to-byzantium-yeats" , ( "" , "" , "" , "" , "" ) )
, ( "#samples" , ( "" , "" , "" , "" , "" ) )
, ( "#section" , ( "" , "" , "" , "" , "" ) )
, ( "#site" , ( "" , "" , "" , "" , "" ) )
, ( "#sonnet-29-shakespeare" , ( "" , "" , "" , "" , "" ) )
, ( "#tao-te-ching" , ( "" , "" , "" , "" , "" ) )
, ( "#the-love-song-of-j.-alfred-prufrock-t.s.-eliot"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#training" , ( "" , "" , "" , "" , "" ) )
, ( "#training-gpt-2-poetry" , ( "" , "" , "" , "" , "" ) )
, ( "#training-gpt-2-poetry-prefix" , ( "" , "" , "" , "" , "" ) )
, ( "#training-gpt-2-small-to-generate-poetry"
  , ( "" , "" , "" , "" , "" )
  )
, ( "#training-samples" , ( "" , "" , "" , "" , "" ) )
, ( "#training-samples-1" , ( "" , "" , "" , "" , "" ) )
, ( "#ulysses-lord-alfred-tennyson" , ( "" , "" , "" , "" , "" ) )
, ( "#unconditional-samples" , ( "" , "" , "" , "" , "" ) )
, ( "/About"
  , ( "About This Website"
    , "shawwn"
    , "shawwn"
    , ""
    , "Meta page describing shawwn.com; copyright license"
    )
  )
, ( "/Faces#fn27"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/Links"
  , ( "Links"
    , "shawwn"
    , "shawwn"
    , ""
    , "Who am I online & what have I done? - Contact information; sites I use; things I've worked on"
    )
  )
, ( "/Resorter"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/TWDNE"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/Tea#water-experiment"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/Tool-AI"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-03-06-gpt2-poetry-1000samples.txt"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-03-06-gpt2-poetry-prefix-1000samples.txt"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-03-16-gpt2-poetry-prefix-jabberwocky-100samples.txt"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-05-13-gpt2-poetry-345m-5000samples.txt"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-05-24-gpt2-poetry-yeatssecondcoming-500completions.txt"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-07-19-taotehching-ch1-1ksamples.txt"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-07-21-taotehching-all-1ksamples.txt"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "/docs/ai/2019-07-22-gpt2-345m-taotehching-all-ch181.tar.xz"
  , ( ""
    , "shawwn"
    , "shawwn"
    , ""
    , "404 Not Found Error: no page by this name!"
    )
  )
, ( "http://antinegationism.tumblr.com/post/182901133106/an-eternal-howl"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://codyraskin.com/research/?p=135"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://creativecommons.org/about/cc0"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://nlp.seas.harvard.edu/2018/04/03/attention.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://papers.nips.cc/paper/6220-memory-efficient-backpropagation-through-time"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://sevensecularsermons.org/on-the-significance-of-gwerns-poem-generator/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://sfbay-anarchists.org/wp-content/uploads/2012/05/Trurls-Electronic-Bard.pdf"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://wiki.obormot.net" , ( "" , "" , "" , "" , "" ) )
, ( "http://www.aclweb.org/anthology/D15-1002"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://www.peterbloem.nl/blog/transformers"
  , ( "" , "" , "" , "" , "" )
  )
, ( "http://yudhanjaya.com/2019/04/the-poetry-machine/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://arxiv.org/abs/1604.06174"
  , ( "Training Deep Nets with Sublinear Memory Cost"
    , "Tianqi Chen, Bing Xu, Chiyuan Zhang, Carlos Guestrin"
    , "2019-08-27"
    , ""
    , "We propose a systematic approach to reduce the memory consumption of deep neural network training. Specifically, we design an algorithm that costs O(sqrt(n)) memory to train a n layer network, with only the computational cost of an extra forward pass per mini-batch. As many of the state-of-the-art models hit the upper bound of the GPU memory, our algorithm allows deeper and more complex models to be explored, and helps advance the innovations in deep learning research. We focus on reducing the memory cost to store the intermediate feature maps and gradients during training. Computation graph analysis is used for automatic in-place operation and memory sharing optimizations. We show that it is possible to trade computation for memory - giving a more memory efficient training algorithm with a little extra computation cost. In the extreme case, our analysis also shows that the memory consumption can be reduced to O(log n) with as little as O(n log n) extra cost for forward computation. Our experiments show that we can reduce the memory cost of a 1,000-layer deep residual network from 48G to 7G with only 30 percent additional running time cost on ImageNet problems. Similarly, significant memory cost reduction is observed in training complex recurrent neural networks on very long sequences."
    )
  )
, ( "https://arxiv.org/abs/1610.01945"
  , ( "Connecting Generative Adversarial Networks and Actor-Critic Methods"
    , "David Pfau, Oriol Vinyals"
    , "2019-08-27"
    , ""
    , "Both generative adversarial networks (GAN) in unsupervised learning and actor-critic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities."
    )
  )
, ( "https://arxiv.org/abs/1611.03852"
  , ( "A Connection between Generative Adversarial Networks, Inverse\n  Reinforcement Learning, and Energy-Based Models"
    , "Chelsea Finn, Paul Christiano, Pieter Abbeel, Sergey Levine"
    , "2019-08-27"
    , ""
    , "Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains."
    )
  )
, ( "https://arxiv.org/abs/1706.03741"
  , ( "Deep reinforcement learning from human preferences"
    , "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, Dario Amodei"
    , "2019-08-27"
    , ""
    , "For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback."
    )
  )
, ( "https://arxiv.org/abs/1706.03762"
  , ( "Attention Is All You Need"
    , "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin"
    , "2019-08-27"
    , ""
    , "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
    )
  )
, ( "https://arxiv.org/abs/1706.03799"
  , ( "Verb Physics: Relative Physical Knowledge of Actions and Objects"
    , "Maxwell Forbes, Yejin Choi"
    , "2019-08-27"
    , ""
    , "Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., \"My house is bigger than me.\" However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, \"Tyler entered his house\" implies that his house is bigger than Tyler.   In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance."
    )
  )
, ( "https://arxiv.org/abs/1706.07068"
  , ( "CAN: Creative Adversarial Networks, Generating \"Art\" by Learning About\n  Styles and Deviating from Style Norms"
    , "Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, Marian Mazzone"
    , "2019-08-27"
    , ""
    , "We propose a new system for generating art. The system generates art by looking at art and learning about style; and becomes creative by increasing the arousal potential of the generated art by deviating from the learned styles. We build over Generative Adversarial Networks (GAN), which have shown the ability to learn to generate novel images simulating a given distribution. We argue that such networks are limited in their ability to generate creative products in their original design. We propose modifications to its objective to make it capable of generating creative art by maximizing deviation from established styles and minimizing deviation from art distribution. We conducted experiments to compare the response of human subjects to the generated art with their response to art created by artists. The results show that human subjects could not distinguish art generated by the proposed system from art generated by contemporary artists and shown in top art fairs. Human subjects even rated the generated images higher on various scales."
    )
  )
, ( "https://arxiv.org/abs/1802.01241"
  , ( "Semantic projection: recovering human knowledge of multiple, distinct\n  object features from word embeddings"
    , "Gabriel Grand, Idan Asher Blank, Francisco Pereira, Evelina Fedorenko"
    , "2019-08-27"
    , ""
    , "The words of a language reflect the structure of the human mind, allowing us to transmit thoughts between individuals. However, language can represent only a subset of our rich and detailed cognitive architecture. Here, we ask what kinds of common knowledge (semantic memory) are captured by word meanings (lexical semantics). We examine a prominent computational model that represents words as vectors in a multidimensional space, such that proximity between word-vectors approximates semantic relatedness. Because related words appear in similar contexts, such spaces - called \"word embeddings\" - can be learned from patterns of lexical co-occurrences in natural language. Despite their popularity, a fundamental concern about word embeddings is that they appear to be semantically \"rigid\": inter-word proximity captures only overall similarity, yet human judgments about object similarities are highly context-dependent and involve multiple, distinct semantic features. For example, dolphins and alligators appear similar in size, but differ in intelligence and aggressiveness. Could such context-dependent relationships be recovered from word embeddings? To address this issue, we introduce a powerful, domain-general solution: \"semantic projection\" of word-vectors onto lines that represent various object features, like size (the line extending from the word \"small\" to \"big\"), intelligence (from \"dumb\" to \"smart\"), or danger (from \"safe\" to \"dangerous\"). This method, which is intuitively analogous to placing objects \"on a mental scale\" between two extremes, recovers human judgments across a range of object categories and properties. We thus show that word embeddings inherit a wealth of common knowledge from word co-occurrence statistics and can be flexibly manipulated to express context-dependent meanings."
    )
  )
, ( "https://arxiv.org/abs/1803.02155"
  , ( "Self-Attention with Relative Position Representations"
    , "Peter Shaw, Jakob Uszkoreit, Ashish Vaswani"
    , "2019-08-27"
    , ""
    , "Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs."
    )
  )
, ( "https://arxiv.org/abs/1808.04444"
  , ( "Character-Level Language Modeling with Deeper Self-Attention"
    , "Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones"
    , "2019-08-27"
    , ""
    , "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions."
    )
  )
, ( "https://arxiv.org/abs/1901.02860"
  , ( "Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context"
    , "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov"
    , "2019-08-27"
    , ""
    , "Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch."
    )
  )
, ( "https://arxiv.org/abs/1904.09751"
  , ( "The Curious Case of Neural Text Degeneration"
    , "Ari Holtzman, Jan Buys, Maxwell Forbes, Yejin Choi"
    , "2019-08-27"
    , ""
    , "Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive.   In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model.   Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence."
    )
  )
, ( "https://arxiv.org/abs/1904.10509"
  , ( "Generating Long Sequences with Sparse Transformers"
    , "Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever"
    , "2019-08-27"
    , ""
    , "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to $O(n \\sqrt{n})$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more."
    )
  )
, ( "https://arxiv.org/abs/1905.02175"
  , ( "Adversarial Examples Are Not Bugs, They Are Features"
    , "Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, Aleksander Madry"
    , "2019-08-27"
    , ""
    , "Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data."
    )
  )
, ( "https://arxiv.org/abs/1905.03197"
  , ( "Unified Language Model Pre-training for Natural Language Understanding\n  and Generation"
    , "Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon"
    , "2019-08-27"
    , ""
    , "This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling objectives: unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. We can fine-tune UniLM as a unidirectional decoder, a bidirectional encoder, or a sequence-to-sequence model to support various downstream natural language understanding and generation tasks.   UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, our model achieves new state-of-the-art results on three natural language generation tasks, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.63 (2.16 absolute improvement), pushing the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), and the SQuAD question generation BLEU-4 to 22.88 (6.50 absolute improvement)."
    )
  )
, ( "https://arxiv.org/abs/1905.12616"
  , ( "Defending Against Neural Fake News"
    , "Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, Yejin Choi"
    , "2019-08-27"
    , ""
    , "Recent progress in natural language generation has raised dual-use concerns. While applications like summarization and translation are positive, the underlying technology also might enable adversaries to generate neural fake news: targeted propaganda that closely mimics the style of real news.   Modern computer security relies on careful threat modeling: identifying potential threats and vulnerabilities from an adversary's point of view, and exploring potential mitigations to these threats. Likewise, developing robust defenses against neural fake news requires us first to carefully investigate and characterize the risks of these models. We thus present a model for controllable text generation called Grover. Given a headline like `Link Found Between Vaccines and Autism,' Grover can generate the rest of the article; humans find these generations to be more trustworthy than human-written disinformation.   Developing robust verification techniques against generators like Grover is critical. We find that best current discriminators can classify neural fake news from real, human-written, news with 73% accuracy, assuming access to a moderate level of training data. Counterintuitively, the best defense against Grover turns out to be Grover itself, with 92% accuracy, demonstrating the importance of public release of strong generators. We investigate these results further, showing that exposure bias -- and sampling strategies that alleviate its effects -- both leave artifacts that similar discriminators can pick up on. We conclude by discussing ethical issues regarding the technology, and plan to release Grover publicly, helping pave the way for better detection of neural fake news."
    )
  )
, ( "https://arxiv.org/abs/1906.08237"
  , ( "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
    , "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le"
    , "2019-08-27"
    , ""
    , "With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, XLNet outperforms BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking."
    )
  )
, ( "https://arxiv.org/abs/1908.04319"
  , ( "Neural Text Generation with Unlikelihood Training"
    , "Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, Jason Weston"
    , "2019-08-27"
    , ""
    , "Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive responses. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model itself are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences that contain repeats and frequent words unlike the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving far superior generations using standard greedy or beam search. Our approach provides a strong alternative to traditional training."
    )
  )
, ( "https://arxiv.org/pdf/1706.03741.pdf#page=15"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://arxiv.org/pdf/1809.11096.pdf#page=6"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://ask-gpt.tumblr.com/" , ( "" , "" , "" , "" , "" ) )
, ( "https://ask-gpt.tumblr.com/post/183402346117/april-is-the-cruelest-month-breeding-n-lilacs"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://blog.acolyer.org/2018/02/22/dynamic-word-embeddings-for-evolving-semantic-discovery/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://blog.floydhub.com/the-transformer-in-pytorch/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://boingboing.net/2019/03/15/digital-lit.html"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://decaut.org/situ/index.php/ttc-compilation/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://en.wikipedia.org/wiki/Amazon_S3"
  , ( "Amazon S3"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Amazon S3</b> or <b>Amazon Simple Storage Service</b> is a service offered by Amazon Web Services (AWS) that provides object storage through a web service interface. Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its global e-commerce network.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Bouba-Kiki_effect"
  , ( "Bouba/kiki effect"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <b>bouba/kiki effect</b> is a non-arbitrary mapping between speech sounds and the visual shape of objects. This effect was first observed by German-American psychologist Wolfgang K\246hler in 1929. In psychological experiments first conducted on the island of Tenerife, K\246hler showed forms similar to those shown at the right and asked participants which shape was called \"takete\" and which was called \"baluba\". Although not explicitly stated, K\246hler implies that there was a strong preference to pair the jagged shape with \"takete\" and the rounded shape with \"baluba\".</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Bradley-Terry_model"
  , ( "Bradley\8211Terry model"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <b>Bradley\8211Terry model</b> is a probability model that can predict the outcome of a paired comparison. Given a pair of individuals <span class=\"texhtml mvar\" style=\"font-style:italic\">i</span> and <span class=\"texhtml mvar\" style=\"font-style:italic\">j</span> drawn from some population, it estimates the probability that the pairwise comparison <span class=\"texhtml \"><i>i</i> &gt; <i>j</i></span> turns out true, as</p><dl><dd><span class=\"mwe-math-element\"><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b702a5b9217ab1a685e9c16743ee8ef98ccf20b5\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align:-2.505ex;width:19.237ex;height:5.509ex\" /></span>\n</dd></dl>"
    )
  )
, ( "https://en.wikipedia.org/wiki/CloudFlare"
  , ( "Cloudflare"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Cloudflare, Inc.</b> is an American web infrastructure and website security company, providing content delivery network services, DDoS mitigation, Internet security, and distributed domain name server services. Cloudflare's services sit between a website's visitor and the Cloudflare user's hosting provider, acting as a reverse proxy for websites. Cloudflare's headquarters are in San Francisco, California, with additional offices in Lisbon, London, Singapore, Munich, San Jose, Champaign, Illinois, Austin, New York City and Washington, D.C.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Content_delivery_network"
  , ( "Content delivery network"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>A <b>content delivery network</b> or <b>content distribution network</b> (<b>CDN</b>) is a geographically distributed network of proxy servers and their data centers. The goal is to provide high availability and high performance by distributing the service spatially relative to end-users. CDNs serve a large portion of the Internet content today, including web objects, downloadable objects, applications, live streaming media, on-demand streaming media, and social media sites.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Creative_Commons"
  , ( "Creative Commons"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Creative Commons</b> (<b>CC</b>) is an American non-profit organization devoted to expanding the range of creative works available for others to build upon legally and to share. The organization has released several copyright-licenses, known as Creative Commons licenses, free of charge to the public. These licenses allow creators to communicate which rights they reserve and which rights they waive for the benefit of recipients or other creators. An easy-to-understand one-page explanation of rights, with associated visual symbols, explains the specifics of each Creative Commons license. Creative Commons licenses do not replace copyright but are based upon it. They replace individual negotiations for specific rights between copyright owner (licensor) and licensee, which are necessary under an \"all rights reserved\" copyright management, with a \"some rights reserved\" management employing standardized licenses for re-use cases where no commercial compensation is sought by the copyright owner. The result is an agile, low-overhead and low-cost copyright-management regime, benefiting both copyright owners and licensees.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Elegy_Written_in_a_Country_Churchyard"
  , ( "Elegy Written in a Country Churchyard"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>Elegy Written in a Country Churchyard</b></i> is a poem by Thomas Gray, completed in 1750 and first published in 1751. The poem's origins are unknown, but it was partly inspired by Gray's thoughts following the death of the poet Richard West in 1742. Originally titled <i>Stanzas Wrote in a Country Church-Yard</i>, the poem was completed when Gray was living near St Giles' parish church at Stoke Poges. It was sent to his friend Horace Walpole, who popularised the poem among London literary circles. Gray was eventually forced to publish the work on 15 February 1751 in order to preempt a magazine publisher from printing an unlicensed copy of the poem.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Essay_On_Criticism"
  , ( "An Essay on Criticism"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>An Essay on Criticism</b></i> is one of the first major poems written by the English writer Alexander Pope (1688\8211\&1744). It is the source of the famous quotations \"To err is human, to forgive divine,\" \"A little learning is a dang'rous thing\", and \"Fools rush in where angels fear to tread.\" It first appeared in 1711 after having been written in 1709, and it is clear from Pope's correspondence that many of the poem's ideas had existed in prose form since at least 1706. Composed in heroic couplets and written in the Horatian mode of satire, it is a verse essay primarily concerned with how writers and critics behave in the new literary commerce of Pope's contemporary age. The poem covers a range of good criticism and advice, and represents many of the chief literary ideals of Pope's age.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Invictus"
  , ( "Invictus"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>\"<b>Invictus</b>\" is a short Victorian poem by the English poet William Ernest Henley (1849\8211\&1903). It was written in 1875 and published in 1888 in his first volume of poems, <i>Book of Verses</i>, in the section <i>Life and Death (Echoes)</i>. It shows how Henley never lost hope and kept faith in himself and faced the struggles unafraid.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Jabberwocky"
  , ( "Jabberwocky"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>\"<b>Jabberwocky</b>\" is a nonsense poem written by Lewis Carroll about the killing of a creature named \"the Jabberwock\". It was included in his 1871 novel <i>Through the Looking-Glass, and What Alice Found There</i>, the sequel to <i>Alice's Adventures in Wonderland</i>. The book tells of Alice's adventures within the back-to-front world of Looking-Glass Land.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Kalevala"
  , ( "Kalevala"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Kalevala</b></i> is a 19th-century work of epic poetry compiled by Elias L\246nnrot from Karelian and Finnish oral folklore and mythology.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Ozymandias"
  , ( "Ozymandias"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>\"Ozymandias\"</b> is the title of two related sonnets published in 1818.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Pioneers%21_O_Pioneers%21"
  , ( "Pioneers! O Pioneers!"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>\"<b>Pioneers! O Pioneers!</b>\" is a poem by the American poet Walt Whitman. It was first published in <i>Leaves of Grass</i> in 1865. The poem was written as a tribute to Whitman's fervor for the great Westward expansion in the United States that led to things like the California Gold Rush and exploration of the far west.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Sonnet_29"
  , ( "Sonnet 29"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Sonnet 29</b> is one of 154 sonnets written by the English playwright and poet William Shakespeare. It is part of the Fair Youth sequence. In the sonnet, the speaker bemoans his status as an outcast and failure but feels better upon thinking of his beloved. Sonnet 29 is written in the typical Shakespearean sonnet form, having 14 lines of iambic pentameter ending in a rhymed couplet.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Tao_Te_Ching"
  , ( "Tao Te Ching"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>The <i><b>Tao Te Ching</b></i>, Chinese: \36947\24503\32463; pinyin: <i><span><b>Dao De Jing</b></span></i>), also known as <i><b>Lao Tzu</b></i> or <i><b>Laozi</b></i>, is a Chinese classic text traditionally credited to the 6th-century BC sage Laozi. The text's authorship, date of composition and date of compilation are debated. The oldest excavated portion dates back to the late 4th century BC, but modern scholarship dates other parts of the text as having been written\8212or at least compiled\8212later than the earliest portions of the <i>Zhuangzi</i>.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/The_Cyberiad"
  , ( "The Cyberiad"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Cyberiad</b></i> is a series of humorous science fiction short stories by Polish writer Stanis\322aw Lem, originally published in 1965, with an English translation appearing in 1974. The main protagonists of the series are Trurl and Klapaucius, the \"constructors\".</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/The_Rime_of_the_Ancient_Mariner"
  , ( "The Rime of the Ancient Mariner"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Rime of the Ancient Mariner</b></i> is the longest major poem by the English poet Samuel Taylor Coleridge, written in 1797\8211\&98 and published in 1798 in the first edition of <i>Lyrical Ballads</i>. Some modern editions use a revised version printed in 1817 that featured a gloss. Along with other poems in <i>Lyrical Ballads</i>, it is often considered a signal shift to modern poetry and the beginning of British Romantic literature.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/The_Song_of_Hiawatha"
  , ( "The Song of Hiawatha"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><i><b>The Song of Hiawatha</b></i> is an 1855 epic poem in trochaic tetrameter by Henry Wadsworth Longfellow which features Native American characters. The epic relates the fictional adventures of an Ojibwe warrior named Hiawatha and the tragedy of his love for Minnehaha, a Dakota woman. Events in the story are set in the Pictured Rocks area on the south shore of Lake Superior. Longfellow's poem is based on oral traditions surrounding the figure of Manabozho, but it also contains his own innovations.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/Thomas_Gray"
  , ( "Thomas Gray"
    , "English Wikipedia"
    , ""
    , ""
    , "<p><b>Thomas Gray</b> was an English poet, letter-writer, classical scholar, and professor at Pembroke College, Cambridge. He is widely known for his <i>Elegy Written in a Country Churchyard,</i> published in 1751.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/beam_search"
  , ( "Beam search"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>In computer science, <b>beam search</b> is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. Beam search is an optimization of best-first search that reduces its memory requirements. Best-first search is a graph search which orders all partial solutions (states) according to some heuristic. But in beam search, only a predetermined number of best partial solutions are kept as candidates. It is thus a greedy algorithm.</p>"
    )
  )
, ( "https://en.wikipedia.org/wiki/liability-threshold_model"
  , ( "Threshold model"
    , "English Wikipedia"
    , ""
    , ""
    , "<p>In mathematical or statistical modeling a <b>threshold model</b> is any model where a threshold value, or set of threshold values, is used to distinguish ranges of values where the behaviour predicted by the model varies in some important way. A particularly important instance arises in toxicology, where the model for the effect of a drug may be that there is zero effect for a dose below a critical or threshold value, while an effect of some significance exists above that value. Certain types of regression model may include threshold effects.</p>"
    )
  )
, ( "https://github.com/ConnorJL/GPT2"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/aparrish/gutenberg-poetry-corpus"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/dariusk/wordfilter"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/google/python-fire/issues/168"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/karpathy/char-rnn/issues/138"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/kylemcdonald/gpt-2-poetry"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/minimaxir/gpt-2-keyword-generation"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/nshepperd/gpt-2"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/openai/gpt-2/blob/master/gpt2-samples.txt"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/openai/gpt-2/issues/19"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://github.com/shawwn/wiki" , ( "" , "" , "" , "" , "" ) )
, ( "https://gpt2.apps.allenai.org/?text=Joel%20is"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://iforcedabot.com/what-can-a-fake-news-detector-do/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://jalammar.github.io/illustrated-transformer/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/@NPCollapse/addendum-evaluation-of-my-model-e6734b51a830"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/@NPCollapse/replicating-gpt2-1-5b-86454a7f26af"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/@mwichary/seduced-shaggy-samson-snored-725b5a8086d9"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/@ysaw/gpt-2-writes-a-shelley-poem-bc0c19fe4ee3"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/huggingface/how-to-build-a-state-of-the-art-conversational-ai-with-transfer-learning-2d818ac26313"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/merzazine/fridai-water-water-everywhere-as-read-by-artificial-intelligence-d02bb3d2b156"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/synapse-dev/understanding-bert-transformer-attention-isnt-all-you-need-5839ebd396db"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!HXhRwS7R!yl4qZM-gMWdn4Qc3scavOBKqdLNAcZ_WYd2gVPqabPg"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!XMl3UI7b!KNLvp5wuxe_WAgJwkMVDiyyNmDl9XDXuipl-dQ6Phow"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!m5FWGCgZ!cjvMgViPbBqep_6HqYDb2D3Kl8Tt-RsUnwg7457IfDk"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://mega.nz/#!zX4lzCzK!TNo_1uDlvszGkBUEdd5R_cQ-7Dfv0gyaaaq8BVzw1jA"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://news.ycombinator.com/item?id=19399467"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://nv-adlr.github.io/MegatronLM"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/better-language-models/#update"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/musenet/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://openai.com/blog/sparse-transformer/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://pali6.github.io/computer-generated-foundation/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://paperswithcode.com/task/language-modelling"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://reddit.com/r/SubSimulatorGPT2Meta/comments/ccvspt/update_experimenting_with_generating_hybrid/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://slatestarcodex.com/2019/02/19/gpt-2-as-step-toward-general-intelligence/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://slatestarcodex.com/2019/03/14/gwerns-ai-generated-poetry/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://stackroboflow.com/" , ( "" , "" , "" , "" , "" ) )
, ( "https://svilentodorov.xyz/blog/gpt-finetune"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://toolbox.google.com/datasetsearch/search?query=poem%20OR%20poetry&docid=T3haTlmLU9Dl6xqYAAAAAA%3D%3D"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://twitter.com/HW" , ( "" , "" , "" , "" , "" ) )
, ( "https://twitter.com/peterkz_swe/status/1098668851640848384"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://twitter.com/robertskmiles/status/1104008764997419009"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://twitter.com/rossgoodwin/status/1124901310677913600"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://twitter.com/theshawwn/lists/machine-learning"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://twitter.com/theshawwn/lists/machine-learning/members"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gutenberg.org/ebooks/24560"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gutenberg.org/ebooks/author/907"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.gwern.net" , ( "" , "" , "" , "" , "" ) )
, ( "https://www.gwern.net/About#design"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.kaggle.com/ultrajack/modern-renaissance-poetry"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.lyrn.ai/2019/01/16/transformer-xl-sota-language-model/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.poetryfoundation.org/harriet/2013/03/the-average-fourth-grader-is-a-better-poet-than-you-and-me-too"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.poetryfoundation.org/poems/45146/to-a-skylark"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.poetryfoundation.org/poems/49303/howl"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/MachineLearning/comments/coc09l/p_these_lyrics_do_not_exist/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/SubSimulatorGPT2/comments/btfhks/what_is_rsubsimulatorgpt2/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/gwern/comments/asb1va/an_eternal_howl_gpt2_completions_of_allen/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/slatestarcodex/comments/as8ke7/an_eternal_howl/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/r/slatestarcodex/comments/b1b47h/gwerns_aigenerated_poetry/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.reddit.com/user/starspawn0"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.thisstorydoesnotexist.com/"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://www.vox.com/2019/5/15/18623134/openai-language-ai-gpt2-poetry-try-it"
  , ( "" , "" , "" , "" , "" )
  )
, ( "https://youtu.be/bIrEM2FbOLU?t=2740"
  , ( "" , "" , "" , "" , "" )
  )
]